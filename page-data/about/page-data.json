{
    "componentChunkName": "component---src-templates-basic-js",
    "path": "/about",
    "result": {"data":{"markdownRemark":{"frontmatter":{"title":"About","path":"/about"},"fields":{"slug":"/basic/about/"},"html":"<p>Deep neural networks are driving many of the recent successes in machine\nlearning. Compared to previous machine learning techniques, it is their ability\nto extract statistical patterns or “features” from data that yields\nstate-of-the-art performance in many domains, such as image recognition or\nnatural language processing.</p>\n<p>From a theoretical point of view however, learning features is only poorly\nunderstood. Indeed, most theoretical works on neural networks either did not\nmodel data structure at all (computer science, statistics) or modeled data as\nindependent, identically distributed (IID) random variables (statistical\nphysics). Despite providing valuable insights, both these approaches were thus\nby construction blind to the statistical properties of real-world datasets, and\nhow they shape learning.</p>\n<p>There is now a growing consensus that understanding deep neural networks will\nrequire a better understanding of the impact of data structure through better\nmodels of data. Indeed, there has been a flurry of activity recently, leading to\nnew tractable models for structured data sets that have already provided a\nnumber of new insights.</p>\n<p>The problem of understanding the impact of data structure is not only relevant\nin the theory of machine learning but also in theoretical neuroscience. Recent\nadvances in large-scale data recording in neuroscience, along with a growing\nappreciation of artificial neural networks as models for biological brains,\nhighlight the need for new theoretical tools to characterise the structure and\nfunction in representations from biological and artificial neural networks. A\ngrowing trend in addressing this challenge is to analyze the geometry of these\nhigh-dimensional representations, i.e., neural population geometry, or \"neural\nmanifolds.\"  Methods based on statistical physics, such as replica theory and\nmean-field theory, have played an essential role in developing theories of\nneural population geometry.</p>\n<p>The <strong>goal of this workshop</strong> is to gather leading scientists from the different\ncommunities that have contributed to these rapidly growing fields. There is a\nlong history going back to the 1980s of fruitful interactions between\ntheoretical neuroscience, machine learning and statistical physics. Our goal is\nto bring together today’s leading researchers from these fields, with a renewed\nfocus on the problems these fields face today. Current questions of interest for\nall these communities are for example the following: How does data structure\nshape the learning dynamics? And how does this structure impact the performance\nof learning? Are there biologically plausible alternatives to training neural\nnetworks with backpropagation? How do neural manifolds impact learning?</p>\n<p>The common theme that unifies these works is the analysis of high-dimensional\nproblems, be they dynamic or static, with the different tools these respective\nfields offer. This convergence of disciplines has previously yielded great\nprogress in signal processing and inference, and we hope that this workshop will\nserve as a focal point for these different communities to come together to\ntackle these new problems centered on neural networks.</p>\n<p>The workshop will be held on February 20-24th 2023 in <a href=\"https://www.houches-school-physics.com/\">Les\nHouches</a> in the French Alps. We are\ncomplementing the workshop with a special issue in the <a href=\"https://iopscience.iop.org/journal/1751-8121\">Journal of Physics\nA</a>, which has a long history of\npublishing articles at the interface between statistical physics and\nneuroscience.</p>"}},"pageContext":{"id":"84bcd3a2-dec5-52d8-847b-efb64e10d6f6"}},
    "staticQueryHashes": ["1820306268","1911626797","239876865","3041911663","4266893431","748283036","760941739"]}